## Decisiones tomadas
He decidido dividir el proyecto en ingestion, transformation y análisis (con el notebook)
La seccion de ingestion cuenta con logReader que lo que busca es simular el cliente de Kafka (por eso es que lee uno por uno los objetos) si contaba con mas tiempo queria montar un producer que sea quien lea el json de eventos, publique los msj y luego el reader escuche el topico y los almacene; aca la decision de ir a un mongoDB es por eso, mi idea es si llegaba con el kafka solo tener responsabilidad de escuchar y almacenar; me quedo acoplado y no es lo que mas feliz me deja el hecho de limpieza y validacion de cada evento, deduplicacion y escritura ya a postgres con los valores flatten; aca lo que pensé es que idealmente esto tiene que ser otro proceso donde lea el mongo y almacene en el postgres o incluso idealmente una BD columnar (redshift, snowflake, etc) en donde este proceso este alineado a las necesidades de freshness de los CU donde puede o no ser streaming, post cada lectura vacia el mongo.
La seccion de transformation toma los eventos de raw y comienza a modelar en una especie de 3fn con arquitectura medallion produciendo trusted que es armar mas el modelado relacional y posterior marts ya con logica de negocio lista para los analisis. Todo esto lo haria con un DBT si quisieramos hacer un proyecto escalable y fuese el core analitico ya que es dificil de mantener dependencias y sincronismo de la manera que lo hice, como asi tambien cambios de nombre, abstracciones, etc; incluso en DBT se podria llevar los test, data contracts, documentacion, semantic layer y todas las features que tiene la tool.
